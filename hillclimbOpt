%grabbed my data from Yahoo finance.
%This cript uses an AI technique called hill climbing. Essentially, the code will start with random weights, and will change them until
%the KL divergence is minimized. 

%Memory Useage:
%because of the random nature of hill climbing, this script could use more memory than your machine has
%to make memory useage less, you can relax the constraints on the weights, or tell the script to jump around less
%to relax the weight constriants, edit the climbWeights() and the updateWeights() functions. Both functions have
%if statements after for loops. To reduce memory useage, make the lower bond on the wight's sum lower (starts at 95).
%keep in mind that this will produce solutions where the fund is not totally invested.
%You can also reudce the number of jumps the code will take. In the hill climb function, 
%change the iof statement for jump to less than 10,000


%starting assets
dow = readtable('DJIA30.csv');
msft = readtable('MSFT.csv');
aapl = readtable('AAPL.csv');

%holds the market, and assets we are investigating
data = {dow, msft, aapl};

%hold weights for portfolio. weights(1) is always the DJIA
weights = [0 0 0];

%will hold distributions of the assets and 
dists = {};

%start with random intial weights
weights = updateWeights(weights);

%finds distributions of individual assets
for i = 1:size(data, 2)
    data{i} = table2array(data{i}(:, 6));
    
    if isa(data{i}, 'double')
        data{i} = num2cell(data{i});
    end
    
    isEm = cellfun(@isempty, data{i}) ;
    tmp = data{i};
    
    if isa(data{i}{1,1}, 'char')
        data{i} = str2double(tmp(~isEm));
        data{i} = tick2ret(data{i});
    else
        data{i} = tick2ret(cell2mat(data{i}));
    end
    
    dist = fitdist(data{i}, 'Normal');
    x = -.2:.00001:.2;
    yKernel = pdf(dist, x);
    dists{i} = yKernel;
end

%creates distribution of the portfolio
portfolioDist = combineDist(dists, weights);

%calculates the divergence
i = 1;
minDiv = [];
minWeight = [];
%while i < 100
    %newDivergence = 0;
    %jumps  = 0;
    %divergence = getKL(yKernel, portfolioDist);
    
    %[minDiv(i), minWeight{i}] = hillClimb(dists, yKernel, weights, divergence, newDivergence, jumps); 
%end

%minDiviation, index = min(minDiv);
%minDiviation;
%minWieght{index}

newDivergence = 0;
jumps  = 0;
divergence = getKL(yKernel, portfolioDist);  %starting divergence
[minDiviation, minWeights] = hillClimb(dists, yKernel, weights, divergence, newDivergence, jumps)  %minimize the divergence

%hill climb to find optimal
function [minDivergence, minWeights] = hillClimb(dists, yKernel, weights, divergence, newDivergence, jumps)
    weights = updateWeights(weights);  %get new weights. Notice use of updateWeights() instead of climbWeights()
    minWeights = weights;  
    newDivergence = getKL(yKernel, combineDist(dists, weights));  %calcualte new diverence
    
    while (newDivergence < divergence)  %while our new divergence calcualtion is less than the previous one...
        minDivergence = newDivergence;
        minWeights = weights;
        divergence = newDivergence;
        weights = climbWeights(weights);  %change weights by a little bit randomly
        newDivergence = getKL(yKernel, combineDist(dists, weights));   %calculate divergence on new weights
    end 
    
    %if the new diverrgence is not better than the last one calcualtes, jump to another point
    jumps = jumps + 1;
    minDivergence = divergence;  %if it is the 10000th jump
    
    if(jumps == 10000)
        return 
    end
    
    hillClimb(dists, yKernel, weights, divergence, newDivergence, jumps);  %hill climb func. again. Because 
    minWeights = weights;  %record answer
    minDivergence = minDivergence;  %record answer
    return 
end

%calculates KL divergence
function divergence = getKL(dist1, dist2)
    logDiff = log2(dist1) - log2(dist2);
    mult = dist1 .* logDiff;
    isNan = isnan(mult);
    mult = mult(~isNan);
    divergence = sum(mult);
end

%calculates the distributions of the combined portfolio
function distribution = combineDist(dists, weights)
    distribution = ones(size(dists{1}, 2), 1);
    for i = 2:size(dists, 2)
        distribution = distribution .* (dists{i} .* weights(i))';
    end
    dist = fitdist(distribution, 'Normal');
    x = -.2:.00001:.2;
    distribution = pdf(dist, x);
end

%jump to another possible weight setting
function weights = updateWeights(weights)
    for i = 2:numel(weights)
            weights(i) = randi([0, 100],1, 1);
    end
    
    if(sum(weights(1, 2:end)) >= 100  || sum(weights(1, 2:end)) < 95)
        updateWeights(weights);
    end
    return
end

%trying to move 'up' a curve as opposed to jumping around
function weights = climbWeights(weights)
    for i = 2:numel(weights)
        weights(i) = weights(i) + (randi([0, 100],1, 1))/100;
    end
    if(sum(weights(1, 2:end)) >= 100 || sum(weights(1, 2:end)) < 95)
        climbWeights(weights);
    end
end
